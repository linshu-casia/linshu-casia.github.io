@article{YANG2026109381,
title = {Bilevel scheduling in downstream oil supply chain: Integrating reinforcement learning with mathematical programming},
journal = {Computers & Chemical Engineering},
volume = {204},
pages = {109381},
year = {2026},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109381},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425003849},
author = {Qipeng Yang and Wentian Fan and Nan Ma and Shu Lin and Jiawen Chang and Zhiqiang Zou and Liang Sun and Haifeng Zhang},
keywords = {Oil supply chain scheduling, Reinforcement learning, Mathematical programming, Rolling-Horizon method, Simulation-base evaluation},
abstract = {With the growth of global energy demand, optimizing the oil supply chain has become crucial. This paper proposes a hybrid reinforcement learning (RL) and mathematical programming (MP) scheduling approach to optimize downstream oil supply chain operations, including refinery production scheduling, logistics distribution, and inventory management. This approach decomposes the complex problem into multiple sub-problems using a Rolling-Horizon method (RH), enhancing computational efficiency and flexibility. We conduct a comparative analysis to evaluate two RL training algorithms with RH: Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) denoted as SAC-RH and PPO-RH respectively. Experimental results from the simulation-based evaluation demonstrate that the SAC version excels in handling complex dynamic environments and continuous action space problems, significantly reducing the number of early warnings and improving overall optimization results. This study demonstrates the applicability of RL in industrial automation and identifies potential avenues for future research.}
}